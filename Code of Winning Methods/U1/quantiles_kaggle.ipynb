{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instructions:\n",
    "#\n",
    "# models will need to run separately in batches (unless you have 64 GB of RAM available):\n",
    "#\n",
    "#    LEVEL = 13, MAX_LEVEL = None  (<1 hour train)\n",
    "#    LEVEL = 14, MAX_LEVEL = None     ibid.\n",
    "#    LEVEL = 15, MAX_LEVEL = None     ibid.\n",
    "#    LEVEL = -1, MAX_LEVEL = 11    (~3 hour train)\n",
    "\n",
    "# you could probably set MAX_LEVEL = 15 and train/infer all at once if you had a lot of RAM \n",
    "\n",
    "# to predict from saved models:\n",
    "#    use each of the above settings with IMPORT = True; (runtime <10 minutes each)\n",
    "\n",
    "# the FINAL_BASE parameter determines whether to forecast the evaluation or validation period\n",
    "\n",
    "# the SPEED = True flag reduces runtimes by 40x and appears to deliver identical performance (0.0% dif in CV)\n",
    "# you may replicate the original submission by setting SPEED = False (200 hours training, 10 hours inference)\n",
    "\n",
    "# turn on REDUCED_FEATURES if you'd like a 30-minute model with 10 features that gets 17th place (~0.170)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEVEL = 0  # Level 13 is HOBBIES; Level 14 is HOUSEHOLD; Level 15 is FOODS (there is no \"Level 12\")\n",
    "MAX_LEVEL = 15\n",
    "IMPORT = False \n",
    "\n",
    "FINAL_BASE = ['d_1941', 'd_1913'][0]\n",
    "\n",
    "\n",
    "SINGLE_FOLD = True\n",
    "SPEED = True\n",
    "SUPER_SPEED = False\n",
    "REDUCED_FEATURES = True\n",
    "\n",
    "sparse_features = ['dayofweek', 'dayofmonth', \n",
    "                     'qs_30d_ewm', 'qs_100d_ewm',\n",
    "                    'qs_median_28d', 'qs_mean_28d',# 'qs_stdev_28d',\n",
    "                    'state_id',\n",
    "               #     'store_id',\n",
    "                   'qs_qtile90_28d',\n",
    "                    'pct_nonzero_days_28d',\n",
    "                    'days_fwd'\n",
    "                    ]\n",
    "\n",
    "LEVEL_SPLITS = [(13, 'HOBBIES'), (14, 'HOUSEHOLD'), (15, 'FOODS')  ]\n",
    "# ID_FILTER = '';   #  ['HOBBIES', 'HOUSEHOLD', 'FOODS', ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUANTILES = [0.005, 0.025, 0.165, 0.25, 0.5,  0.75, 0.835, 0.975, 0.995]  \n",
    "# QUANTILES = [0.25, 0.5, 0.75]\n",
    "# QUANTILES = [0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_DICT = \\\n",
    "    {1: (0.3, 0.7),   2: (0.1, 0.7),  3: (0.1, 0.5), \n",
    "     4: (0.3, 0.5),   5: (0.15, 1),    6: (0.2, 0.5),\n",
    "     7: (0.1, 1),     8: (0.2, 0.5),    9: (0.1, 0.5),\n",
    "    10: (0.05, 0.5), 11: (0.04, 1),  \n",
    "    13: (0.12, 2),            14: (0.065, 2),          15: (0.03, 0.5)}\n",
    "#     'HOBBIES': (0.12, 2), 'HOUSEHOLD': (0.065, 2), 'FOODS': (0.03, 0.5)}\n",
    "\n",
    "\n",
    "SS_SS = 0.8    # 0.8 was production version ***\n",
    "\n",
    "if SPEED or SUPER_SPEED or REDUCED_FEATURES:\n",
    "    SS_SS /= (5 if SUPER_SPEED else (2 if SPEED else 1)) * (5 if REDUCED_FEATURES else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BAGS = 1\n",
    "N_JOBS = -1\n",
    "\n",
    "SS_PWR = 0.6\n",
    "BAGS_PWR = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE_DROPS = ['item_id', '_abs_diff', 'squared_diff' ]\\\n",
    "                +    ['336', '300d'] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run-time parameters\n",
    "CACHED_FEATURES = False\n",
    "CACHE_FEATURES = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIME_SEED = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Packages and Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lightgbm in /home/artemis/.local/lib/python3.7/site-packages (2.3.1)\n",
      "Requirement already satisfied: numpy in /home/artemis/anaconda3/lib/python3.7/site-packages (from lightgbm) (1.18.1)\n",
      "Requirement already satisfied: scipy in /home/artemis/anaconda3/lib/python3.7/site-packages (from lightgbm) (1.4.1)\n",
      "Requirement already satisfied: scikit-learn in /home/artemis/anaconda3/lib/python3.7/site-packages (from lightgbm) (0.22.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/artemis/anaconda3/lib/python3.7/site-packages (from scikit-learn->lightgbm) (0.14.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import datetime as datetime\n",
    "from scipy.stats.mstats import gmean\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import gzip\n",
    "import bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = (17,5.5)\n",
    "rcParams['figure.max_open_warning'] = 0\n",
    "# %config InlineBackend.figure_format='retina'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 150\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.datetime.now()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TIME_SEED:\n",
    "    np.random.seed(datetime.datetime.now().microsecond)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "def sizeof_fmt(num, suffix='B'):\n",
    "    ''' by Fred Cirera,  https://stackoverflow.com/a/1094933/1870254, modified'''\n",
    "    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n",
    "        if abs(num) < 1024.0:\n",
    "            return \"%3.1f %s%s\" % (num, unit, suffix)\n",
    "        num /= 1024.0\n",
    "    return \"%.1f %s%s\" % (num, 'Yi', suffix)\n",
    "\n",
    "def memCheck():\n",
    "    for name, size in sorted(((name, sys.getsizeof(value)) for name, value in globals().items()),\n",
    "                             key= lambda x: -x[1])[:10]:\n",
    "        print(\"{:>30}: {:>8}\".format(name, sizeof_fmt(size)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ramCheck():\n",
    "    print(\"{:.1f} GB used\".format(psutil.virtual_memory().used/1e9 - 0.7))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/kaggle/input/m5-forecasting-uncertainty/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.9 GB used\n"
     ]
    }
   ],
   "source": [
    "ramCheck()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Aggregate Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEVELS = [(12, ['item_id', 'store_id']),\n",
    "          (11, ['state_id', 'item_id']),\n",
    "          (10, ['item_id']),\n",
    "          (9, ['store_id', 'dept_id']),\n",
    "          (8, ['store_id', 'cat_id']),\n",
    "          (7, ['state_id', 'dept_id']),\n",
    "          (6, ['state_id', 'cat_id']),\n",
    "          (5, ['dept_id']),\n",
    "          (4, ['cat_id']),\n",
    "          (3, ['store_id']),\n",
    "          (2, ['state_id']),\n",
    "          (1, []) ]\n",
    "\n",
    "DOWNSTREAM = {'item_id': ['dept_id', 'cat_id'],\n",
    "              'dept_id': ['cat_id'],\n",
    "              'store_id': ['state_id']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggTrain(train):\n",
    "    tcd = dict([(col, 'first') for col in train.columns[1:6]])\n",
    "    tcd.update( dict([(col, 'sum') for col in train.columns[6:]]))\n",
    "\n",
    "    tadds =[]; tadd_levels= [ [12 for i in range(0, len(train))] ] \n",
    "    for idx, lvl in enumerate(LEVELS[1:]):\n",
    "        level = lvl[0]\n",
    "        lvls = lvl[1]\n",
    "\n",
    "        if len(lvls) is 0:  # group all if no list provided\n",
    "            lvls = [1 for i in range(0, len(train))]\n",
    "\n",
    "        tadd = train.groupby(lvls).agg(tcd)\n",
    "\n",
    "        # name it\n",
    "        if len(lvls) == 2:\n",
    "            tadd.index = ['_'.join(map(str,i)) for i in tadd.index.tolist()]\n",
    "        elif len(lvls) == 1:\n",
    "            tadd.index = tadd.index + '_X'\n",
    "        else:\n",
    "            tadd.index = ['Total_X']\n",
    "        tadd.index.name = 'id'\n",
    "\n",
    "        # fill in categorical features\n",
    "        tadd.reset_index(inplace=True)\n",
    "        for col in [c for c in train.columns[1:6] if c not in lvls and not  \n",
    "                            any(c in z for z in[DOWNSTREAM[lvl] for lvl in lvls if lvl in DOWNSTREAM])]:\n",
    "            tadd[col] = 'All'\n",
    "        tadds.append(tadd)\n",
    "\n",
    "        #levels\n",
    "        tadd_levels.append([level for i in range(0, len(tadd))])\n",
    "\n",
    "    train = pd.concat((train,*tadds), sort=False, ignore_index=True); del tadds, tadd\n",
    "    levels = pd.Series(data = [x for sub_list in tadd_levels for x in sub_list], index = train.index); del tadd_levels\n",
    "    for col in train.columns[1:6]:\n",
    "        train[col] = train[col].astype('category')\n",
    "        \n",
    "    return train, levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadTrain():\n",
    "    train_cols =  pd.read_csv(path+ '/' + 'sales_train_evaluation.csv', nrows=1)\n",
    "\n",
    "    c_dict = {}\n",
    "    for col in [c for c in train_cols if 'd_' in c]:\n",
    "        c_dict[col] = np.float32\n",
    "\n",
    "    train = pd.read_csv(path+ '/' + 'sales_train_evaluation.csv', dtype=c_dict)#.astype(np.int16, errors='ignore')\n",
    "\n",
    "    train.id = train.id.str.split('_').str[:-1].str.join('_')\n",
    "    \n",
    "    train.sort_values('id', inplace=True)\n",
    "    \n",
    "    return train.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPricePivot():\n",
    "    prices = pd.read_csv(path+ '/' + 'sell_prices.csv',\n",
    "                    dtype = {'wm_yr_wk': np.int16, 'sell_price': np.float32})\n",
    "    prices['id'] = prices.item_id + \"_\" + prices.store_id\n",
    "    price_pivot =  prices.pivot(columns = 'id' , index='wm_yr_wk', values = 'sell_price')\n",
    "    price_pivot = price_pivot.reindex(sorted(price_pivot.columns), axis=1)\n",
    "    return price_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCal():\n",
    "    return pd.read_csv(path+ '/' + 'calendar.csv').set_index('d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File /kaggle/input/m5-forecasting-uncertainty//calendar.csv does not exist: '/kaggle/input/m5-forecasting-uncertainty//calendar.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-788d412c6ad3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetCal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mday_to_cal_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcal_index_to_day\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-dd50de5407b4>\u001b[0m in \u001b[0;36mgetCal\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgetCal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'calendar.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'd'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1891\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File /kaggle/input/m5-forecasting-uncertainty//calendar.csv does not exist: '/kaggle/input/m5-forecasting-uncertainty//calendar.csv'"
     ]
    }
   ],
   "source": [
    "cal = getCal()\n",
    "cal.date = pd.to_datetime(cal.date)\n",
    "\n",
    "day_to_cal_index = dict([(col, idx) for idx, col in enumerate(cal.index)])\n",
    "cal_index_to_day = dict([(idx, col) for idx, col in enumerate(cal.index)])\n",
    "\n",
    "cal_index_to_wm_yr_wk = dict([(idx, col) for idx, col in enumerate(cal.wm_yr_wk)])\n",
    "day_to_wm_yr_wk = dict([(idx, col) for idx, col in cal.wm_yr_wk.iteritems()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load\n",
    "train = loadTrain()\n",
    "price_pivot = getPricePivot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total Time Elapsed: ', (datetime.datetime.now() - start).seconds, 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine\n",
    "assert (train.id == price_pivot.columns).all()\n",
    "daily_sales = pd.concat((train.iloc[:, :6], \n",
    "                        train.iloc[:, 6:] * price_pivot.loc[train.columns[6:].fillna(0)\\\n",
    "                                                                .map(day_to_wm_yr_wk)].transpose().values ), \n",
    "                            axis = 'columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate\n",
    "train, levels = aggTrain(train)\n",
    "# id_to_level = dict(zip(train.id, levels))\n",
    "# level_to_ids = dict([(level[0], list(train.id[levels == level[0]])) for idx, level in enumerate(LEVELS)])\n",
    "\n",
    "daily_sales = aggTrain(daily_sales)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total Time Elapsed: ', (datetime.datetime.now() - start).seconds, 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rescale each level to avoid hitting np.half ceiling and keep similar ranges\n",
    "level_multiplier = dict([ (c, (levels==c).sum() / (levels==12).sum()) for c in sorted(levels.unique())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split up level 12\n",
    "for row in LEVEL_SPLITS:\n",
    "    level_multiplier[row[0]] = level_multiplier[12]\n",
    "    levels.loc[(levels == 12) & (train.cat_id == row[1])] = row[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(levels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rescale by number of series at each level\n",
    "train = pd.concat((train.iloc[:, :6], \n",
    "                        train.iloc[:, 6:].multiply( levels.map(level_multiplier), axis = 'index').astype(np.float32) ), \n",
    "                            axis = 'columns')\n",
    "\n",
    "daily_sales = pd.concat((daily_sales.iloc[:, :6], \n",
    "                        daily_sales.iloc[:, 6:].multiply( levels.map(level_multiplier), axis = 'index').astype(np.float32) ), \n",
    "                            axis = 'columns')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadSampleSub():\n",
    "    return pd.read_csv(path+ '/' + 'sample_submission.csv').astype(np.int8, errors = 'ignore')\n",
    "\n",
    "sample_sub = loadSampleSub()\n",
    "\n",
    "assert set(train.id) == set(sample_sub.id.str.split('_').str[:-2].str.join('_'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ramCheck()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# memCheck()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total Time Elapsed: ', (datetime.datetime.now() - start).seconds, 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filter = (   \n",
    "               ( ( MAX_LEVEL is not None )   & (levels <= MAX_LEVEL) )  | \n",
    "               (  ( MAX_LEVEL is None )  &  (levels == LEVEL) )\n",
    "                 )\n",
    "train = train[train_filter].reset_index(drop=True)\n",
    "daily_sales = daily_sales[train_filter].reset_index(drop=True)\n",
    "levels = levels[train_filter].reset_index(drop=True).astype(np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(levels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_head = train.iloc[:, :6]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_head.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ramCheck()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace leading zeros with nan\n",
    "train['d_1'].replace(0, np.nan, inplace=True)\n",
    "\n",
    "for i in range(train.columns.get_loc('d_1') + 1, train.shape[1]):\n",
    "    train.loc[:, train.columns[i]].where( ~ ((train.iloc[:,i]==0) & (train.iloc[:,i-1].isnull())),\n",
    "                                         np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total Time Elapsed: ', (datetime.datetime.now() - start).seconds, 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_flipped = train.set_index('id', drop = True).iloc[:, 5:].transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_flipped.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_flipped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_flipped.max().sort_values(ascending=False)[::3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# memCheck()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total Time Elapsed: ', (datetime.datetime.now() - start).seconds, 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ramCheck()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total Time Elapsed: ', (datetime.datetime.now() - start).seconds, 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item-Store Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic moving averages\n",
    "if not CACHED_FEATURES:      \n",
    "    for window in [3, 7, 15, 30, 100]:\n",
    "        if REDUCED_FEATURES and window < 15: continue;\n",
    "        features.append(('qs_{}d_ewm'.format(window), \n",
    "                         train_flipped.ewm(span=window, \n",
    "                                           min_periods = int(np.ceil(window ** 0.8))  ).mean().astype(np.half)))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_avg_qs = train_flipped[train_flipped.columns[levels >= 12]].transpose()\\\n",
    "            .groupby(train_head.iloc[(levels >= 12).values].store_id.values).mean().fillna(1)\n",
    "store_dept_avg_qs = train_flipped[train_flipped.columns[levels >= 12]].transpose()\\\n",
    "            .groupby(  ( train_head.iloc[(levels >= 12).values].store_id.astype(str) + '_'\n",
    "                        + train_head.iloc[(levels >= 12).values].dept_id.astype(str)).values\n",
    "                    ).mean().fillna(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_avg_qs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic moving averages, after removing any store trends\n",
    "scaled_sales = train_flipped / (store_avg_qs.loc[train.store_id].transpose().values); \n",
    "\n",
    "# if levels.min() == 12:\n",
    "#     # get overall store and store-dept sales matched to this id;\n",
    "#     store_avg_qs_matched = store_avg_qs.loc[train.store_id].transpose() \n",
    "#     store_dept_avg_qs_matched = store_dept_avg_qs.loc[train.store_id.astype(str) + '_'\n",
    "#                                                   + train.dept_id.astype(str)\n",
    "#                                                 ].transpose() \n",
    "\n",
    "#     store_avg_qs_matched.columns = train_flipped.columns\n",
    "#     store_dept_avg_qs_matched.columns = train_flipped.columns\n",
    "\n",
    "#     ratio = (store_avg_qs_matched.rolling(28).mean() / store_avg_qs_matched.rolling(56).mean() ) .fillna(1) - 1\n",
    "#     ratio = ratio.clip ( ratio.stack().quantile(0.01), ratio.stack().quantile(0.99))\n",
    "# #     features.append(('store_28d_58d_ratio',  ratio.astype(np.half)))\n",
    "\n",
    "#     ratio = (store_dept_avg_qs_matched.rolling(28).mean() / store_dept_avg_qs_matched.rolling(56).mean() ) .fillna(1) - 1\n",
    "#     ratio = ratio.clip ( ratio.stack().quantile(0.003), ratio.stack().quantile(0.997))\n",
    "\n",
    "# #     features.append(('store_dept_28d_58d_ratio',  ratio.astype(np.half)))\n",
    "\n",
    "#     del store_avg_qs_matched, store_dept_avg_qs_matched, ratio\n",
    "\n",
    "del store_avg_qs, store_dept_avg_qs,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# moving average after store-level detrending\n",
    "if not CACHED_FEATURES:\n",
    "    for window in [3, 7, 15, 30, 100]:\n",
    "        if REDUCED_FEATURES: continue;\n",
    "        features.append(('qs_divbystore_{}d_ewm'.format(window), \n",
    "                         scaled_sales.ewm(span=window,\n",
    "                                           min_periods = int(np.ceil(window ** 0.8))  ).mean().astype(np.half)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total Time Elapsed: ', (datetime.datetime.now() - start).seconds, 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EWM % NONZERO DAYS\n",
    "if not CACHED_FEATURES:\n",
    "    tff0ne0 = train_flipped.fillna(0).ne(0)\n",
    "    for window in [7, 14, 28, 28*2, 28*4,  ]:  \n",
    "        if REDUCED_FEATURES and window != 28: continue;\n",
    "        features.append( ('pct_nonzero_days_{}d'.format(window),\n",
    "                         tff0ne0.rolling(window).mean().astype(np.half) ) )\n",
    "    del tff0ne0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total Time Elapsed: ', (datetime.datetime.now() - start).seconds, 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features for Both Sales and Scaled Sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrs = [train_flipped, scaled_sales, ] # sales_over_all]\n",
    "labels = ['qs', 'qs_divbystore', ] #'qs_divbyall']\n",
    "\n",
    "if REDUCED_FEATURES: arrs = arrs[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic lag features\n",
    "if not CACHED_FEATURES:\n",
    "    for lag in range(1, 10+1):\n",
    "        if REDUCED_FEATURES: continue;\n",
    "        features.append( ('qs_lag_{}d'.format(lag),\n",
    "                              train_flipped.shift(lag).fillna(0).astype(np.half) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# means and medians -- by week to avoid day of week effects\n",
    "\n",
    "if not CACHED_FEATURES:\n",
    "    for idx in range(0, len(arrs)):\n",
    "        arr = arrs[idx]\n",
    "        label = labels[idx]\n",
    "\n",
    "        for window in [7, 14, 21, 28, 28*2, 28*4,  ]:  ## ** mean and median\n",
    "            if REDUCED_FEATURES and window != 28: continue;\n",
    "            features.append( ('{}_mean_{}d'.format(label, window), \n",
    "                          arr.rolling(window).mean().astype(np.half) )  )\n",
    "\n",
    "            features.append( ('{}_median_{}d'.format(label, window), \n",
    "                          arr.rolling(window).median().astype(np.half) )  )\n",
    "            \n",
    "            print('{}: {}'.format(label,window))\n",
    "\n",
    "        del arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total Time Elapsed: ', (datetime.datetime.now() - start).seconds, 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stdev, skewness, and kurtosis\n",
    "# ideally kurtosis and skewness should NOT be labeled qs_ as they are scale-invariant\n",
    "\n",
    "if not CACHED_FEATURES:\n",
    "    for idx in range(0, len(arrs)):\n",
    "        arr = arrs[idx]\n",
    "        label = labels[idx]\n",
    "        for window in [7, 14, 28, 28*3, 28*6]:\n",
    "            if REDUCED_FEATURES and window != 28: continue;\n",
    "            print('{}: {}'.format(label,window))\n",
    "\n",
    "            features.append( ('{}_stdev_{}d'.format(label, window), \n",
    "                                  arr.rolling(window).std().astype(np.half) )  )\n",
    "\n",
    "            if window >= 10:\n",
    "                if REDUCED_FEATURES: continue;\n",
    "                features.append( ('{}_skew_{}d'.format(label, window), \n",
    "                                      arr.rolling(window).skew().astype(np.half) )  )\n",
    "\n",
    "                features.append( ('{}_kurt_{}d'.format(label, window), \n",
    "                                      arr.rolling(window).kurt().astype(np.half) )  )\n",
    "\n",
    "    del arr;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total Time Elapsed: ', (datetime.datetime.now() - start).seconds, 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# high and low quantiles (adding more seemed to hurt performance)\n",
    "\n",
    "if not CACHED_FEATURES:\n",
    "    for idx in range(0, len(arrs)):\n",
    "        arr = arrs[idx]\n",
    "        label = labels[idx]\n",
    "        for window in [14, 28, 56]:\n",
    "            if REDUCED_FEATURES and window != 28: continue;\n",
    "\n",
    "            features.append( ('{}_qtile10_{}d'.format(label, window), \n",
    "                          arr.rolling(window).quantile(0.1).astype(np.half) )  )\n",
    "\n",
    "            features.append( ('{}_qtile90_{}d'.format(label, window), \n",
    "                          arr.rolling(window).quantile(0.9).astype(np.half) )  )\n",
    "\n",
    "            print('{}: {}'.format(label,window))\n",
    "        del arr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total Time Elapsed: ', (datetime.datetime.now() - start).seconds, 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del arrs; del scaled_sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ramCheck()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start after one year, remove anything with proximity to holiday months (given mid-year LB targets)\n",
    "# also saves a lot of RAM/processing time \n",
    "\n",
    "def clean_df(fr):\n",
    "    early_rows = cal[cal.year == cal.year.min()].index.to_list()\n",
    "    holiday_rows = cal[cal.month.isin([10, 11, 12, 1])].index.to_list()\n",
    "    delete_rows = early_rows + holiday_rows\n",
    "    \n",
    "    MIN_DAY = 'd_{}'.format(300)\n",
    "    \n",
    "    if 'd' in fr.columns: # d, series stack:\n",
    "        fr = fr[fr.d >= day_to_cal_index[MIN_DAY]]\n",
    "        fr = fr[~fr.d.isin([  day_to_cal_index[d] for d in delete_rows])]\n",
    "        \n",
    "        \n",
    "    else:  # pivot table\n",
    "        if MIN_DAY in fr.index:\n",
    "            fr = fr.iloc[ fr.index.get_loc(MIN_DAY):, :]\n",
    "\n",
    "        if len(delete_rows) > 0:\n",
    "            fr = fr[~fr.index.isin(delete_rows)]\n",
    "    \n",
    "    return fr;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_features(features):\n",
    "    for idx, feat_row in enumerate(features):\n",
    "        fr = feat_row[1]\n",
    "        fr = clean_df(fr)\n",
    "\n",
    "        if len(fr) < len(feat_row[1]):\n",
    "            features[idx] = (features[idx][0], fr)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ramCheck()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total Time Elapsed: ', (datetime.datetime.now() - start).seconds, 's')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cache Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_dir = '/kaggle/input/m5-e300/'\n",
    "\n",
    "if CACHED_FEATURES:\n",
    "    if 'features.pbz2' in os.listdir(pickle_dir):\n",
    "        with bz2.BZ2File(pickle_dir + 'features.pbz2', 'r') as handle:\n",
    "            features = pickle.load(handle)\n",
    "    elif 'features.pgz' in os.listdir(pickle_dir):\n",
    "        with gzip.GzipFile(pickle_dir + 'features.pgz', 'r') as handle:\n",
    "            features = pickle.load(handle)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ramCheck()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total Time Elapsed: ', (datetime.datetime.now() - start).seconds, 's')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_features(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_features(item_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total Time Elapsed: ', (datetime.datetime.now() - start).seconds, 's')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ramCheck()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CACHE_FEATURES:\n",
    "    with gzip.GzipFile('features.pgz', 'w') as handle:\n",
    "        pickle.dump(features, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    os.path.getsize('features.pgz') / 1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total Time Elapsed: ', (datetime.datetime.now() - start).seconds, 's')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total Time Elapsed: ', (datetime.datetime.now() - start).seconds, 's')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calendar Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_features = pd.DataFrame()\n",
    "\n",
    "cal_features['dayofweek'] =  cal.date.dt.dayofweek.astype(np.int8)\n",
    "cal_features['dayofmonth'] =  cal.date.dt.day.astype(np.int8)\n",
    "cal_features['season'] =  cal.date.dt.month.astype(np.half)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State Calendar Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_cal_features = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snap_cols = [c for c in cal.columns if 'snap' in c]\n",
    "\n",
    "state_cal_features.append( ( 'snap_day' , \n",
    "                                cal[snap_cols].astype(np.int8) ) )\n",
    "state_cal_features.append( ( 'snap_day_lag_1' , \n",
    "                                cal[snap_cols].shift(1).fillna(0).astype(np.int8) ) )\n",
    "state_cal_features.append( ( 'snap_day_lag_2' , \n",
    "                                cal[snap_cols].shift(2).fillna(0).astype(np.int8) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_cal_features.append( ( 'nth_snap_day',\n",
    "            (cal[snap_cols].rolling(15, min_periods = 1).sum() * cal[snap_cols] ).astype(np.int8)  ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for window in [2, 5, 10, 30, 60]:\n",
    "    state_cal_features.append( ('snap_{}d_ewm'.format(window),\n",
    "                                    cal[snap_cols].ewm(span = window, adjust=False).mean().astype(np.half) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# strip columns to match state_id\n",
    "def snapRename(x):\n",
    "    return x.replace('snap_', '')\n",
    "\n",
    "for f in range(0, len(state_cal_features)):\n",
    "    state_cal_features[f] = (state_cal_features[f][0],\n",
    "                                state_cal_features[f][1].rename(snapRename, axis = 'columns')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.merge( pd.Series(np.sum(train_flipped, axis = 1), name='total_sales'), cal, \n",
    "#          left_index=True, right_index=True).groupby('event_name_2').mean()\\\n",
    "#                 .sort_values('total_sales', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for etype in [c for c in cal.event_type_1.dropna().unique()]:\n",
    "    cal[etype.lower() + '_holiday'] = np.where(cal.event_type_1 == etype,\n",
    "                                       cal.event_name_1,\n",
    "                                               np.where(cal.event_type_2 == etype,\n",
    "                                                    cal.event_name_2, 'None'))\n",
    "\n",
    "for etype in [c for c in cal.event_type_1.dropna().unique()]:\n",
    "    cal[etype.lower() + '_holiday'] = cal[etype.lower() + '_holiday'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Price Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPricePivot():\n",
    "    prices = pd.read_csv(path+ '/' + 'sell_prices.csv',\n",
    "                    dtype = {'wm_yr_wk': np.int16, 'sell_price': np.float32})\n",
    "    prices['id'] = prices.item_id + \"_\" + prices.store_id\n",
    "    price_pivot =  prices.pivot(columns = 'id' , index='wm_yr_wk', values = 'sell_price')\n",
    "    return price_pivot\n",
    "\n",
    "\n",
    "price_pivot = getPricePivot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ramCheck()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# memCheck()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assemble Series-Features Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series_to_series_id = dict([(col, idx) for idx, col in enumerate(train_flipped.columns)])\n",
    "series_id_to_series = dict([(idx, col) for idx, col in enumerate(train_flipped.columns)])\n",
    "series_id_level = dict([(idx, col) for idx, col in enumerate(levels)])\n",
    "series_level = dict(zip(train_flipped.columns, levels))\n",
    "\n",
    "series_to_item_id = dict([(x[1].id, x[1].item_id) for x in train_head[['id', 'item_id']].iterrows()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in features:\n",
    "    assert feature[1].shape == features[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fstack = features[0][1].stack(dropna = False)\n",
    "series_features = pd.DataFrame({'d': fstack.index.get_level_values(0) \\\n",
    "                                                .map(day_to_cal_index).values.astype(np.int16),\n",
    "                     'series': fstack.index.get_level_values(1) \\\n",
    "                                                .map(series_to_series_id).values.astype(np.int16)  })\n",
    "del fstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, feature in enumerate(features):\n",
    "    if feature is not None:\n",
    "        series_features[feature[0]] = feature[1].stack(dropna=False).values\n",
    "        \n",
    "del features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ramCheck()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### State Cal Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in state_cal_features:\n",
    "    assert feature[1].shape == state_cal_features[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fstack = state_cal_features[0][1].stack(dropna = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_cal_series_features = pd.DataFrame({'d': fstack.index.get_level_values(0) \\\n",
    "                                                .map(day_to_cal_index).values.astype(np.int16),\n",
    "                     'state': fstack.index.get_level_values(1)  })\n",
    "del fstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, feature in enumerate(state_cal_features):\n",
    "    if feature is not None:\n",
    "        state_cal_series_features[feature[0]] = feature[1].stack(dropna=False).values\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean Up NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series_features.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series_features.fillna(-10, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add Categoricals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORICALS = ['dept_id', 'cat_id', 'store_id', 'state_id', ] # 'item_id'] # never item_id; wrecks higher layers;\n",
    "\n",
    "        \n",
    "for col in CATEGORICALS:\n",
    "    series_features[col] = series_features.series.map(series_id_to_series).map(\n",
    "                train_head.set_index('id')[col]) #.astype('category')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ramCheck()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# memCheck()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total Time Elapsed: ', (datetime.datetime.now() - start).seconds, 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics and Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addSuffix(c):\n",
    "    return c + '_validation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trailing_28d_sales = daily_sales.iloc[:,6:].transpose().rolling(28, min_periods = 1).sum().astype(np.float32)\n",
    "\n",
    "fstack = train_flipped.stack(dropna = False)\n",
    "weight_stack = pd.DataFrame({'d': fstack.index.get_level_values(0) \\\n",
    "                                                .map(day_to_cal_index).values.astype(np.int16),\n",
    "                     'series': fstack.index.get_level_values(1) \\\n",
    "                                                .map(series_to_series_id).values.astype(np.int16),\n",
    "                    'days_since_first': (~train_flipped.isnull()).expanding().sum().stack(dropna = False).values\\\n",
    "                                             .astype(np.int16),\n",
    "                    'trailing_vol': ( (train_flipped.diff().abs()).expanding().mean() ).astype(np.float16)\\\n",
    "                                                 .stack(dropna = False).values,\n",
    "                    'weights': (trailing_28d_sales / \n",
    "                                    trailing_28d_sales.transpose().groupby(levels).sum().loc[levels].transpose().values)\n",
    "                                     .astype(np.float16)\\\n",
    "                                             .stack(dropna = False).values,\n",
    "                            })\n",
    "\n",
    "del fstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del trailing_28d_sales; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_stack.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_items = weight_stack.days_since_first < 30\n",
    "weight_stack[new_items].weights.sum() / weight_stack[weight_stack.days_since_first >= 0].weights.sum()\n",
    "weight_stack.loc[new_items, 'weights'] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ramCheck()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total Time Elapsed: ', (datetime.datetime.now() - start).seconds, 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Weight and Y into Main Df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_stack = clean_df(weight_stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(weight_stack) == len(series_features)\n",
    "assert (weight_stack.d.values == series_features.d).all()\n",
    "assert (weight_stack.series.values == series_features.series).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series_features = pd.concat( (series_features, \n",
    "                weight_stack.reset_index(drop=True).iloc[:, -2:]), axis = 1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_stack = weight_stack.iloc[:10, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fstack = train_flipped.stack(dropna = False)\n",
    "y_full = pd.DataFrame({'d': fstack.index.get_level_values(0) \\\n",
    "                                                .map(day_to_cal_index).values.astype(np.int16),\n",
    "                     'series': fstack.index.get_level_values(1) \\\n",
    "                                                .map(series_to_series_id).values.astype(np.int16),\n",
    "                      'y': fstack.values})\n",
    "del fstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ramCheck()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# memCheck()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Merges to Build X/Y/etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addMAcrosses(X):\n",
    "    EWMS = [c for c in X.columns if 'ewm' in c and 'qs_' in c and len(c) < 12]\n",
    "    for idx1, col1 in enumerate(EWMS):\n",
    "        for idx2, col2 in enumerate(EWMS):\n",
    "            if not idx1 < idx2:\n",
    "                continue;\n",
    "            \n",
    "            X['qs_{}_{}_ewm_diff'.format(col1.split('_')[1], col2.split('_')[1])] = X[col1] - X[col2]\n",
    "            X['qs_{}_{}_ewm_ratio'.format(col1.split('_')[1], col2.split('_')[1])] = X[col1] / X[col2]\n",
    "                \n",
    "    return X\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addCalFeatures(X):  # large block of code; easy;\n",
    "    # day of week, month, season of year\n",
    "    X['dayofweek'] = ( X.d + X.days_fwd).map(cal_index_to_day).map(cal_features.dayofweek)\n",
    "    X['dayofmonth'] = ( X.d + X.days_fwd).map(cal_index_to_day).map(cal_features.dayofmonth)\n",
    " \n",
    "    X['basedayofweek'] = X.d.map(cal_index_to_day).map(cal_features.dayofweek)\n",
    "    X['dayofweekchg'] = (X.days_fwd % 7).astype(np.int8)\n",
    "\n",
    "    X['basedayofmonth'] = X.d.map(cal_index_to_day).map(cal_features.dayofmonth)\n",
    "    X['season'] =  ( ( X.d + X.days_fwd).map(cal_index_to_day).map(cal_features.season) \\\n",
    "                             + np.random.normal( 0, 1, len(X)) ).astype(np.half)\n",
    "                        # with a full month SD of noise to not overfit to specific days;\n",
    "\n",
    "    # holidays\n",
    "    holiday_cols = [c for c in cal.columns if '_holiday' in c]\n",
    "    for col in holiday_cols:\n",
    "        X['base_' + col] = X.d.map(cal_index_to_day).map(cal[col])\n",
    "        X[col] = ( X.d + X.days_fwd).map(cal_index_to_day).map(cal[col])\n",
    "\n",
    "    \n",
    "    return X\n",
    "#     'dayofweek'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertToLinearFeatures(X):\n",
    "    X = X.copy()\n",
    "    for s in X.dayofweek.unique():\n",
    "        X['dayofweek_{}'.format(s)] = (X.dayofweek == s).astype(np.int8)\n",
    "    X.drop( columns = X.columns[X.dtypes == 'category'], inplace=True)\n",
    "    X['daysfwd_sqrt'] = (X.days_fwd ** 0.5).astype(np.half)\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addStateCalFeatures(X):  \n",
    "    if (X.state_id == 'All').mean() > 0:\n",
    "        print('No State Ids')\n",
    "        return X;\n",
    "    \n",
    "    def rename_scf(c, name = 'basedate'):\n",
    "        return c if (c=='d' or c == 'state') else name + '_' + c\n",
    "    \n",
    "    X['future_d'] = ( X.d + X.days_fwd)\n",
    "    X['state'] = X.state_id.astype('object')\n",
    "    \n",
    "    nX = X.merge(state_cal_series_features[['state', 'd', 'snap_day', 'nth_snap_day']]\n",
    "                 .rename(rename_scf, axis = 'columns'),\n",
    "                                         on = ['d', 'state'],  \n",
    "             validate='m:1', how = 'inner', suffixes = (False, False)) \n",
    "    \n",
    "    \n",
    "    nX = nX.merge(state_cal_series_features[['state', 'd', 'snap_day', 'nth_snap_day']]\n",
    "                 .rename(columns = {'d': 'future_d'}), \n",
    "                                         on = ['future_d', 'state'],  \n",
    "             validate='m:1', how = 'inner', suffixes = (False, False)) \n",
    "    \n",
    "    nX.drop(columns = ['state', 'future_d'], inplace=True)\n",
    "    \n",
    "    assert len(nX) == len(X)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return nX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_item_features(X):  \n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATION = -1; # 2016 # pure holdout from train and prediction sets;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getXYG(X, scale_range = None, oos = False):\n",
    "    start_time = datetime.datetime.now(); \n",
    "\n",
    "    # ensure it's in the train set, and days_forward is actually *forward*\n",
    "    X.drop( X.index[ (X.days_fwd < 1) |\n",
    "           (  ~oos  &  ( X.d + X.days_fwd > cal.index.get_loc(train_flipped.index[-1])  )    ) ], inplace=True)\n",
    "    g = gc.collect()\n",
    "    \n",
    "    \n",
    "    X = addMAcrosses(X)\n",
    "\n",
    "    X = addCalFeatures(X)\n",
    "    X = addStateCalFeatures(X)\n",
    "    \n",
    "    # noise to time-static features\n",
    "    for col in [c for c in X.columns if 'store' in c and 'ratio' in c]:\n",
    "        X[col] = X[col] + np.random.normal(0, 0.1, len(X))\n",
    "        print('adding noise to {}'.format(col))\n",
    "    \n",
    "\n",
    "    # match with Y\n",
    "    if 'y' not in X.columns:\n",
    "        st = datetime.datetime.now(); \n",
    "        X['future_d'] = X.d + X.days_fwd\n",
    "        if oos:  \n",
    "            X = X.merge(y_full.rename(columns = {'d': 'future_d'}), on = ['future_d', 'series'], \n",
    "                             how = 'left')\n",
    "            X.y = X.y.fillna(-1)\n",
    "            \n",
    "        else:  \n",
    "            X = X.merge(y_full.rename(columns = {'d': 'future_d'}), on = ['future_d', 'series'],\n",
    "                       )#    suffixes = (None, None), validate = 'm:1')\n",
    "#     X['yo'] = X.y.copy()\n",
    "    g = gc.collect()\n",
    "    \n",
    "    scaler_columns = [c for c in X.columns if c in weight_stack.columns[2:]]\n",
    "    scalers = X[scaler_columns].copy()\n",
    "    y = X.y\n",
    "    \n",
    "    groups = pd.Series(cal.iloc[(X.d + X.days_fwd)].year.values, X.index).astype(np.int16)\n",
    "    \n",
    "    \n",
    "    # feature drops\n",
    "    if REDUCED_FEATURES:\n",
    "        feat_drops = [c for c in X.columns if c not in (sparse_features + ['d', 'series', 'days_fwd'])]\n",
    "    \n",
    "    elif len(FEATURE_DROPS) > 0:\n",
    "        feat_drops = [c for c in X.columns if any(z in c for z in FEATURE_DROPS )]\n",
    "        print('dropping {} features; anything containing {}'.format(len(feat_drops), FEATURE_DROPS))\n",
    "        print('   -- {}'.format(feat_drops))\n",
    "    else:\n",
    "        feat_drops = []\n",
    "        \n",
    "    # final drops\n",
    "    X.drop(columns = scaler_columns + (['future_d'] if 'future_d' in X.columns else []) + ['y'] + feat_drops , inplace=True)\n",
    "\n",
    "    scalers['scaler'] = scalers.trailing_vol.copy()\n",
    "    \n",
    "    # randomize scaling\n",
    "    if scale_range > 0:\n",
    "        scalers.scaler = scalers.scaler * np.exp( scale_range * ( np.random.normal(0, 0.5, len(X))) )\n",
    "#         scalers.scaler = scalers.scaler * np.exp( scale_range * ( np.random.rand(len(X)) - 0.5) )\n",
    "    \n",
    "    # now rescale y and  'scaled variable' in X by its vol\n",
    "    for col in [c for c in X.columns if 'qs_' in c and 'ratio' not in c]:\n",
    "        X[col] = np.where( X[col] == -10, X[col], (X[col] / scalers.scaler).astype(np.half)) \n",
    "    y = y / scalers.scaler\n",
    "    \n",
    "    \n",
    "    yn = (oos == False) & (y.isnull() | (groups==VALIDATION)) \n",
    "\n",
    "    \n",
    "    print(\"\\nXYG Pull Time: {}\".format(str(datetime.datetime.now() - start_time).split('.', 2)[0] ))\n",
    "    \n",
    "    return (X[~yn], y[~yn], groups[~yn], scalers[~yn])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(k, v) for k, v in series_id_level.items() if v == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSubsample(frac, level = 12, scale_range = 0.1, n_repeats = 1, drops = True, post_process_X = None):\n",
    "    start_time = datetime.datetime.now(); \n",
    "\n",
    "    wtg_mean = series_features.weights[(series_features.series.map(series_id_level) == level)].mean()\n",
    "    ss = series_features.weights / wtg_mean * frac\n",
    "    print(ss)\n",
    "    X = series_features[  (ss > np.random.rand(len(ss)) ) \n",
    "                              & (series_features.series.map(series_id_level) == level) ]\n",
    "    ss =  X.weights / wtg_mean   * frac \n",
    "    print(X.shape)  \n",
    "    print('{} series that seek oversampling'.format( (ss > 1). sum() ) )\n",
    "    print( ss[ss>1].sort_values()[-5:])\n",
    "    \n",
    "    extras = []\n",
    "    \n",
    "    while ss.max() > 1:\n",
    "        ss = ss - 1\n",
    "        extras.append( X[ ss > np.random.rand(len(ss))] )\n",
    "        \n",
    "    if len(extras) > 0:\n",
    "        print(' scaled EWMS of extras:')\n",
    "        print( ( extras[-1].qs_30d_ewm / extras[-1].trailing_vol)[-5:] )\n",
    "\n",
    "    if len(extras) > 0:\n",
    "        X = pd.concat((X, *extras))\n",
    "    else:\n",
    "        X = X.copy()\n",
    "    \n",
    "    \n",
    "    X['days_fwd'] = (np.random.randint(0, 28, size = len(X)) + 1).astype(np.int8)\n",
    "    \n",
    "    if n_repeats > 1:\n",
    "         X = pd.concat([X] * n_repeats)\n",
    "\n",
    "    g = gc.collect()\n",
    "    print(X.shape)\n",
    "    X, y, groups, scalers = getXYG(X, scale_range)\n",
    "    ramCheck()\n",
    "    g = gc.collect()\n",
    "    if drops:\n",
    "        X.drop(columns = ['d', 'series'], inplace=True)\n",
    "    \n",
    "    if post_process_X is not None:\n",
    "        X = post_process_X(X)\n",
    "    \n",
    "    print(X.shape)\n",
    "    print(\"\\nSubsample Time: {}\\n\".format(str(datetime.datetime.now() - start_time).split('.', 2)[0] ))\n",
    "\n",
    "    return X, y, groups, scalers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total Time Elapsed: ', (datetime.datetime.now() - start).seconds, 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV, GroupKFold, LeaveOneGroupOut\n",
    "from sklearn.model_selection import ParameterSampler\n",
    "from sklearn.metrics import make_scorer\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantile_loss(true, pred, quantile = 0.5):\n",
    "    loss = np.where(true >= pred, \n",
    "                        quantile*(true-pred),\n",
    "                        (1-quantile)*(pred - true) )\n",
    "    return np.mean(loss)   \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantile_scorer(quantile = 0.5):\n",
    "    return make_scorer(quantile_loss, False, quantile = quantile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_quantile_params = {     # fairly well tuned, with high runtimes \n",
    "                'max_depth': [10, 20],\n",
    "                'n_estimators': [   200, 300, 350, 400, ],   \n",
    "                'min_split_gain': [0, 0, 0, 0, 1e-4, 1e-3, 1e-2, 0.1],\n",
    "                'min_child_samples': [ 2, 4, 7, 10, 14, 20, 30, 40, 60, 80, 100, 130, 170, 200, 300, 500, 700, 1000 ],\n",
    "                'min_child_weight': [0, 0, 0, 0, 1e-4, 1e-3, 1e-3, 1e-3, 5e-3, 2e-2, 0.1 ],\n",
    "                'num_leaves': [ 20, 30, 30, 30, 50, 70, 90, ],\n",
    "                'learning_rate': [  0.02, 0.03, 0.04, 0.04, 0.05, 0.05, 0.07, ],         \n",
    "                'colsample_bytree': [0.3, 0.5, 0.7, 0.8, 0.9, 0.9, 0.9, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \n",
    "                'colsample_bynode':[0.1, 0.15, 0.2, 0.2, 0.2, 0.25, 0.3, 0.5, 0.65, 0.8, 0.9, 1],\n",
    "                'reg_lambda': [0, 0, 0, 0, 1e-5, 1e-5, 1e-5, 1e-5, 3e-5, 1e-4, 1e-3, 1e-2, 0.1, 1, 10, 100   ],\n",
    "                'reg_alpha': [0, 1e-5, 3e-5, 1e-4, 1e-4, 1e-3, 3e-3, 1e-2, 0.1, 1, 1, 10, 10, 100, 1000,],\n",
    "                'subsample': [  0.9, 1],\n",
    "                'subsample_freq': [1],\n",
    "                'cat_smooth': [0.1, 0.2, 0.5, 1, 2, 5, 7, 10],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SPEED or SUPER_SPEED or REDUCED_FEATURES:\n",
    "    lgb_quantile_params = {     # fairly well tuned, with high runtimes \n",
    "                'max_depth': [10, 20],\n",
    "                'n_estimators': [ 150, 200, 200],  # 300, 350, 400, ],   \n",
    "                'min_split_gain': [0, 0, 0, 0, 1e-4, 1e-3, 1e-2, 0.1],\n",
    "                'min_child_samples': [ 2, 4, 7, 10, 14, 20, 30, 40, 60, 80, 100, 100, 100, \n",
    "                                                  130, 170, 200, 300, 500, 700, 1000 ],\n",
    "                'min_child_weight': [0, 0, 0, 0, 1e-4, 1e-3, 1e-3, 1e-3, 5e-3, 2e-2, 0.1 ],\n",
    "                'num_leaves': [ 20, 30, 50, 50 ], # 50, 70, 90, ],\n",
    "                'learning_rate': [  0.04, 0.05, 0.07, 0.07, 0.07, 0.1, 0.1, 0.1 ],   # 0.02, 0.03,        \n",
    "                'colsample_bytree': [0.3, 0.5, 0.7, 0.8, 0.9, 0.9, 0.9, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \n",
    "                'colsample_bynode':[0.1, 0.15, 0.2, 0.2, 0.2, 0.25, 0.3, 0.5, 0.65, 0.8, 0.9, 1],\n",
    "                'reg_lambda': [0, 0, 0, 0, 1e-5, 1e-5, 1e-5, 1e-5, 3e-5, 1e-4, 1e-3, 1e-2, 0.1, 1, 10, 100   ],\n",
    "                'reg_alpha': [0, 1e-5, 3e-5, 1e-4, 1e-4, 1e-3, 3e-3, 1e-2, 0.1, 1, 1, 10, 10, 100, 1000,],\n",
    "                'subsample': [  0.9, 1],\n",
    "                'subsample_freq': [1],\n",
    "                'cat_smooth': [0.1, 0.2, 0.5, 1, 2, 5, 7, 10],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainLGBquantile(x, y, groups, cv = 0, n_jobs = -1, alpha = 0.5, **kwargs):\n",
    "    clfargs = kwargs.copy(); clfargs.pop('n_iter', None)\n",
    "    clf = lgb.LGBMRegressor(verbosity=-1, hist_pool_size = 1000,  objective = 'quantile', alpha = alpha,\n",
    "                            importance_type = 'gain',\n",
    "                            seed = datetime.datetime.now().microsecond if TIME_SEED else None,\n",
    "                             **clfargs,\n",
    "                      )\n",
    "    print('\\n\\n Running Quantile Regression for \\u03BC={}\\n'.format(alpha))\n",
    "    params = lgb_quantile_params\n",
    "    \n",
    "    return trainModel(x, y, groups, clf, params, quantile_scorer(alpha), n_jobs, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(x, y, groups, clf, params, cv = 0, n_jobs = None, \n",
    "                   verbose=0, splits=None, **kwargs):\n",
    "    if n_jobs is None:\n",
    "        n_jobs = -1\n",
    "    folds = LeaveOneGroupOut()\n",
    "    clf = RandomizedSearchCV(clf, params, cv=  folds, \n",
    "                             n_iter= ( kwargs['n_iter'] if len(kwargs) > 0 and 'n_iter' in kwargs else 4), \n",
    "                            verbose = 0, n_jobs = n_jobs, scoring = cv)\n",
    "    f = clf.fit(x, y, groups)\n",
    "    print(pd.DataFrame(clf.cv_results_['mean_test_score'])); print();  \n",
    "\n",
    "    best = clf.best_estimator_;  print(best)\n",
    "    print(\"\\nBest In-Sample CV: {}\\n\".format(np.round(clf.best_score_,4)))\n",
    "\n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runQBags(n_bags = 3, model_type = trainLGBquantile, data = None, quantiles = [0.5], **kwargs):\n",
    "    start_time = datetime.datetime.now(); \n",
    "    \n",
    "    clf_set = []; loss_set = []\n",
    "    for bag in range(0, n_bags):\n",
    "        print('\\n\\n  Running Bag {} of {}\\n\\n'.format(bag+1, n_bags))\n",
    "        if data is None:\n",
    "            X, y, groups, scalers = getSubsample()\n",
    "        else:\n",
    "            X, y, groups, scalers = data\n",
    "\n",
    "        group_list = [*dict.fromkeys(groups)]   \n",
    "        group_list.sort()\n",
    "        print(\"Groups: {}\".format(group_list))\n",
    "\n",
    "        clfs = []; preds = []; ys=[]; datestack = []; losses = pd.DataFrame(index=QUANTILES)\n",
    "        if SINGLE_FOLD: group_list = group_list[-1:]\n",
    "        for group in group_list:\n",
    "            print('\\n\\n   Running Models with {} Out-of-Fold\\n\\n'.format(group))\n",
    "            x_holdout = X[groups == group]\n",
    "            y_holdout = y[groups == group]\n",
    "            \n",
    "            ramCheck()\n",
    "            model = model_type \n",
    "            \n",
    "            q_clfs = []; q_losses = []\n",
    "            for quantile in quantiles:\n",
    "                set_filter = (groups != group) \\\n",
    "                        & (np.random.rand(len(groups)) < \n",
    "                                 quantile_wts[quantile] ** (0.35 if LEVEL >=11 else 0.25) )\n",
    "                clf = model(X[set_filter], y[set_filter], groups[set_filter], \n",
    "                                alpha = quantile, **kwargs) \n",
    "                q_clfs.append(clf)\n",
    "\n",
    "                predicted = clf.predict(x_holdout)\n",
    "\n",
    "                q_losses.append((quantile, quantile_loss(y_holdout, predicted, quantile)))\n",
    "                print(u\"{} \\u03BC={:.3f}: {:.4f}\".format(group, quantile, q_losses[-1][1] ) )\n",
    "                \n",
    "                preds.append(predicted)\n",
    "                ys.append(y_holdout)\n",
    "            \n",
    "            clfs.append(q_clfs)\n",
    "            print(\"\\nLevel {} OOS Losses for Bag {} in {}:\".format(level, bag+1, group))\n",
    "            print(np.round(pd.DataFrame(q_losses).set_index(0)[1], 4))\n",
    "            losses[group] = np.round(pd.DataFrame(q_losses).set_index(0)[1], 4).values\n",
    "            print(\"\\nElapsed Time So Far This Bag: {}\\n\".format(str(datetime.datetime.now() - start_time).split('.', 2)[0] ))\n",
    "            \n",
    "        \n",
    "        clf_set.append(clfs)\n",
    "        print(\"\\nLevel {} Year-by-Year OOS Losses for Bag {}:\".format(level, bag, group))\n",
    "        print(losses)\n",
    "        \n",
    "        loss_set.append(losses)\n",
    "        print(\"\\nModel Bag Time: {}\\n\".format(str(datetime.datetime.now() - start_time).split('.', 2)[0] ))\n",
    "    return clf_set, loss_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "level_os = dict([(idx, 1/val) for (idx,val) in level_multiplier.items()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are to use less processing time on edge quantiles \n",
    "QUANTILE_LEVELS = [0.005, 0.025, 0.165, 0.25, 0.5, 0.75, 0.835, 0.975, 0.995]\n",
    "QUANTILE_WTS = [0.1, 0.2, 0.6, 0.8, 1, 0.9, 0.7, 0.2, 0.1,]\n",
    "    \n",
    "quantile_wts = dict(zip(QUANTILE_LEVELS, QUANTILE_WTS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total Time Elapsed: ', (datetime.datetime.now() - start).seconds, 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actually Run Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not IMPORT:\n",
    "    clf_set = {}; loss_set = {}; LEVEL_QUANTILES = {};\n",
    "    for level in sorted(levels.unique()):\n",
    "        print(\"\\n\\n\\nRunning Models for Level {}\\n\\n\\n\".format(level))\n",
    "        \n",
    "        SS_FRAC, SCALE_RANGE = P_DICT[level] # if level < 12 else ID_FILTER]; \n",
    "        SS_FRAC = SS_FRAC * SS_SS\n",
    "        print('{}/{}'.format(SS_FRAC, SCALE_RANGE))\n",
    "        \n",
    "        # much higher iteration counts for low levels\n",
    "        clf_set[level], loss_set[level] = runQBags(n_bags = int(BAGS * level_os[level] ** BAGS_PWR), \n",
    "                                                   model_type = trainLGBquantile, \n",
    "                                                   data = getSubsample(SS_FRAC * level_os[level] ** SS_PWR, \n",
    "                                                                       level, SCALE_RANGE),\n",
    "                                                        n_iter =  int( \n",
    "                                                                 (2.2 if level <= 9 else 1.66) \n",
    "                                                                   * (16 - (level if level <=12 else 12) ) \n",
    "                                                                    * (1/4 if SUPER_SPEED else (1/2 if SPEED else 1))   \n",
    "                                                                     ) ,\n",
    "                      quantiles = QUANTILES,\n",
    "                       n_jobs = N_JOBS) \n",
    "        \n",
    "        LEVEL_QUANTILES[level] = QUANTILES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getSubsample(SS_FRAC * level_os[level] ** SS_PWR, \n",
    "                                                                       level, SCALE_RANGE), "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total Time Elapsed: ', (datetime.datetime.now() - start).seconds, 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IMPORT:\n",
    "    clf_sets = []  # ***\n",
    "    path = '/kaggle/input/m5clfs/'\n",
    "    \n",
    "   # if LEVEL != 12: \n",
    "    files = [f for f in os.listdir(path) if '.pkl' in f]\n",
    "    if LEVEL == 13 and MAX_LEVEL is None: files = [f for f in files if '13_' in f or 'hobbies' in f]\n",
    "    if LEVEL == 14 and MAX_LEVEL is None: files = [f for f in files if '14_' in f or 'household' in f]\n",
    "    if LEVEL == 15 and MAX_LEVEL is None: files = [f for f in files if '15_' in f or 'foods' in f]      \n",
    "        \n",
    "  #  else:\n",
    "  #      files = [f for f in os.listdir(path) if '.pkl' in f and ID_FILTER.lower() in f]\n",
    "        \n",
    "    for file in files:\n",
    "        clf_sets.append(pickle.load(open(path + file,'rb')))\n",
    " \n",
    "    clf_df = []; pairs = []\n",
    "    for clf_set in clf_sets:\n",
    "        for level, level_clfs in clf_set.items():\n",
    "            for clf_bag_idx, clf_bag in enumerate(level_clfs):\n",
    "                for group_idx, clf_group in enumerate(clf_bag):\n",
    "                    for quantile_idx, clf in enumerate(clf_group):\n",
    "                        clf_df.append((level, clf.alpha, group_idx, clf))\n",
    "\n",
    "\n",
    "    clf_df = pd.DataFrame(clf_df, columns = ['level', 'alpha', 'group', 'clf'])\n",
    "    \n",
    "    if LEVEL > 12 and MAX_LEVEL == None:\n",
    "        clf_df.loc[clf_df.level==12, 'level'] = LEVEL\n",
    "\n",
    "\n",
    "    # clf_df\n",
    "    \n",
    "    LEVEL_QUANTILES = {}; clf_set = {}\n",
    "    for level in sorted(clf_df.level.unique()):\n",
    "\n",
    "        level_df = clf_df[clf_df.level == level]\n",
    "\n",
    "        level_list = []\n",
    "        for group in sorted(level_df.group.unique()):\n",
    "            group_df = level_df[level_df.group == group].sort_values('alpha')\n",
    "            if level in LEVEL_QUANTILES:\n",
    "                assert LEVEL_QUANTILES[level] == list(group_df.alpha)\n",
    "            else:\n",
    "                LEVEL_QUANTILES[level] = list(group_df.alpha)\n",
    "            level_list.append(list(group_df.clf))\n",
    "        if len(level_df.group.unique()) > 1:\n",
    "            SINGLE_FOLD = False\n",
    "        clf_set[level] = [level_list]\n",
    "        print(level, \": \", LEVEL_QUANTILES[level]); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LEVEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for level in sorted(clf_set.keys()):\n",
    "    print(\"Level {}:\".format(level))\n",
    "    \n",
    "    for idx, q in enumerate(LEVEL_QUANTILES[level]):\n",
    "        print(u'\\n\\n      Regressors for \\u03BC={}:\\n'.format(q))\n",
    "        for clf in [q_clfs[idx] for clfs in clf_set[level] for q_clfs in clfs]:\n",
    "            print(clf)\n",
    "    \n",
    "    print(); print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total Time Elapsed: ', (datetime.datetime.now() - start).seconds, 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save classifiers\n",
    "clf_file = ('clf_set.pkl' if IMPORT \n",
    "                          else ('lvl_{}_clfs.pkl'.format(LEVEL) if MAX_LEVEL == None \n",
    "                                                            else 'lvls_lt_{}_clfs.pkl'.format(MAX_LEVEL)))\n",
    "with open(clf_file, 'wb') as handle:\n",
    "    pickle.dump(clf_set, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ramCheck()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_FI(model, featNames, featCount):\n",
    "   # show_FI_plot(model.feature_importances_, featNames, featCount)\n",
    "    fis = model.feature_importances_\n",
    "    fig, ax = plt.subplots(figsize=(6, 5))\n",
    "    indices = np.argsort(fis)[::-1][:featCount]\n",
    "    g = sns.barplot(y=featNames[indices][:featCount],\n",
    "                    x = fis[indices][:featCount] , orient='h' )\n",
    "    g.set_xlabel(\"Relative importance\")\n",
    "    g.set_ylabel(\"Features\")\n",
    "    g.tick_params(labelsize=12)\n",
    "    g.set_title( \" feature importance\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_FI(all_clfs, featNames, featCount, title = \"Feature Importances\"):\n",
    "    # 1. Sum\n",
    "    clfs = []\n",
    "    for clf_set in all_clfs:\n",
    "        for clf in clf_set:\n",
    "            clfs.append(clf);\n",
    "    fi = np.zeros( (len(clfs), len(clfs[0].feature_importances_)) )\n",
    "    for idx, clf in enumerate(clfs):\n",
    "        fi[idx, :] = clf.feature_importances_\n",
    "    avg_fi = np.mean(fi, axis = 0)\n",
    "\n",
    "    # 2. Plot\n",
    "    fis = avg_fi\n",
    "    fig, ax = plt.subplots(figsize=(6, 5))\n",
    "    indices = np.argsort(fis)[::-1]#[:featCount]\n",
    "    #print(indices)\n",
    "    g = sns.barplot(y=featNames[indices][:featCount],\n",
    "                    x = fis[indices][:featCount] , orient='h' )\n",
    "    g.set_xlabel(\"Relative importance\")\n",
    "    g.set_ylabel(\"Features\")\n",
    "    g.tick_params(labelsize=12)\n",
    "    g.set_title(title + ' - {} classifiers'.format(len(clfs)))\n",
    "    \n",
    "    return pd.Series(fis[indices], featNames[indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_FI_plot(fi, featNames, featCount):\n",
    "   # show_FI_plot(model.feature_importances_, featNames, featCount)\n",
    "    fig, ax = plt.subplots(figsize=(6, 5))\n",
    "    indices = np.argsort(np.absolute(fi))[::-1]#[:featCount]\n",
    "    g = sns.barplot(y=featNames[indices][:featCount],\n",
    "                    x = fi[indices][:featCount] , orient='h' )\n",
    "    g.set_xlabel(\"Relative importance\")\n",
    "    g.set_ylabel(\"Features\")\n",
    "    g.tick_params(labelsize=12)\n",
    "    g.set_title( \" feature importance\")\n",
    "    return pd.Series(fi[indices], featNames[indices])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for level in sorted(clf_set.keys()):\n",
    "    X = getSubsample(0.0001, level, 0.1)[0]\n",
    "    print(\"Level {}:\".format(level))\n",
    "    for idx, q in enumerate(LEVEL_QUANTILES[level]):\n",
    "        f = avg_FI([[q_clfs[idx] for clfs in clf_set[level] for q_clfs in clfs]], X.columns, 25, \n",
    "                       title = \"Level {} \\u03BC={} Feature Importances\".format(level, q))\n",
    "    print(); print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ramCheck()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total Time Elapsed: ', (datetime.datetime.now() - start).seconds, 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg(arr, axis = 0):\n",
    "    return np.median(arr, axis = axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictSet(X, y, groups, scalers, clf_set):\n",
    "    start_time = datetime.datetime.now(); \n",
    "    \n",
    "    group_list = [*dict.fromkeys(groups)]   \n",
    "    group_list.sort()\n",
    "#     print(group_list)\n",
    "    \n",
    "    y_unscaled = y * scalers.scaler\n",
    "    \n",
    "    all_preds = []; ys=[]; gs = []; xs = []; scaler_stack = []\n",
    "    if SINGLE_FOLD: group_list = group_list[-1:]\n",
    "    for group_idx, group in enumerate(group_list):\n",
    "        g = gc.collect()\n",
    "        x_holdout = X[groups == group]\n",
    "        y_holdout = y_unscaled[groups == group] \n",
    "        scalers_holdout = scalers[groups == group]\n",
    "        groups_holdout = groups[groups == group]\n",
    "        \n",
    "        preds = np.zeros( (len(QUANTILES), len(y_holdout)), dtype=np.half)\n",
    "        for q_idx, quantile in enumerate(QUANTILES):            \n",
    "            q_preds = np.zeros( ( len(clf_set), len(y_holdout) ) )\n",
    "            for bag_idx, clf in enumerate(clf_set):\n",
    "                x_clean = x_holdout.drop(columns = [c for c in x_holdout.columns if c=='d' or c=='series'])\n",
    "                if group_idx >= len(clf_set[bag_idx]): # if out of sample year, blend all years\n",
    "                    qs_preds = np.zeros( (group_idx, len(x_clean)) )\n",
    "                    for gidx in range(group_idx):\n",
    "                        qs_preds[gidx, :] = clf_set[bag_idx][gidx][q_idx].predict(x_clean)\n",
    "                    q_preds[bag_idx, :] = np.mean(qs_preds, axis = 0)\n",
    "                else:\n",
    "                    q_preds[bag_idx, :] = clf_set[bag_idx][group_idx][q_idx].predict(x_clean)\n",
    "                \n",
    "            q_preds = avg(q_preds) * scalers_holdout.scaler\n",
    "\n",
    "            preds[q_idx, :] = q_preds\n",
    "            \n",
    "#             print(u\"{} \\u03BC={:.3f}: {:.4f}\".format(group, quantile, quantile_loss(y_holdout, q_preds, quantile) ) )\n",
    "        \n",
    "        all_preds.append(preds)\n",
    "        xs.append(x_holdout)\n",
    "        ys.append(y_holdout)\n",
    "        gs.append(groups_holdout)\n",
    "        scaler_stack.append(scalers_holdout)\n",
    "        print()\n",
    "    y_pred = np.hstack(all_preds)\n",
    "    scaler_stack = pd.concat(scaler_stack)\n",
    "    y_true = pd.concat(ys)\n",
    "    groups = pd.concat(gs)\n",
    "    X = pd.concat(xs)\n",
    "    \n",
    "    end_time = datetime.datetime.now(); \n",
    "    print(\"Bag Prediction Time: {}\".format(str(end_time - start_time).split('.', 2)[0] ))\n",
    "    return y_pred, y_true, groups, scaler_stack, X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictOOS(X, scalers, clf_set, QUANTILES, validation = False):\n",
    "    start_time = datetime.datetime.now(); \n",
    "    \n",
    "    group_list = [1 + i for i in range(0, len(clf_set[0]))]   \n",
    "    if validation:\n",
    "        group_list = np.zeros(len(clf_set[0]))\n",
    "        group_list[-1] = 1\n",
    "    \n",
    "    \n",
    "    divisor = sum(group_list)\n",
    "    print(np.round([g / divisor for g in group_list], 3)); print()\n",
    "    \n",
    "    x_holdout = X\n",
    "    scalers_holdout = scalers \n",
    "\n",
    "    preds = np.zeros( (len(clf_set[0][0]), len(x_holdout)), dtype=np.float32)\n",
    "    for q_idx in range( len(clf_set[0][0])): # loop over quantiles\n",
    "        print(u'Predicting for \\u03BC={}'.format( QUANTILES[q_idx]) )\n",
    "        \n",
    "        q_preds = np.zeros( ( len(clf_set), len(x_holdout) ), dtype = np.float32 )\n",
    "        for bag_idx, clf in enumerate(clf_set):\n",
    "            x_clean = x_holdout # .drop(columns = [c for c in x_holdout.columns if c=='d' or c=='series'])\n",
    "            qs_preds = np.zeros( (len(group_list), len(x_clean)), dtype = np.float32 )\n",
    "            if SINGLE_FOLD: group_list = group_list[-1:]\n",
    "            for gidx in range(len(group_list)):\n",
    "                if group_list[gidx] > 0: \n",
    "                    qs_preds[gidx, :] = clf_set[bag_idx][gidx][q_idx].predict(x_clean) * group_list[gidx] / divisor\n",
    "            q_preds[bag_idx, :] = np.sum(qs_preds, axis = 0)\n",
    "\n",
    "        q_preds = np.mean(q_preds, axis = 0) * scalers_holdout.scaler\n",
    "\n",
    "        preds[q_idx, :] = q_preds\n",
    " \n",
    "    end_time = datetime.datetime.now(); \n",
    "    print(\"Bag Prediction Time: {}\".format(str(end_time - start_time).split('.', 2)[0] ))\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wspl(true, pred, weights, trailing_vol, quantile = 0.5):\n",
    "    loss = weights * np.where(true >= pred, \n",
    "                        quantile*(true-pred),\n",
    "                        (1-quantile)*(pred - true) ) / trailing_vol\n",
    "    return np.mean(loss) / np.mean(weights)   \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Sample Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATION = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RSEED = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of samples for each data point;\n",
    "N_REPEATS = 20 #if LE <15 else 10  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qls = {}; all_predictions = {}\n",
    "for level in sorted(set(clf_set.keys()) & set(levels)):\n",
    "    print(\"\\n\\n\\nLevel {}\\n\\n\\n\".format(level))\n",
    "    QUANTILES = LEVEL_QUANTILES[level]\n",
    "    \n",
    "    SS_FRAC, SCALE_RANGE = P_DICT[level] #  if level < 12 else ID_FILTER]; \n",
    "    SS_FRAC = SS_FRAC * SS_SS \n",
    "    EVAL_FRAC = SS_FRAC * (1 if level < 11 else 1/2) \n",
    "    EVAL_PWR = 0.6\n",
    "    SCALE_RANGE_TEST = SCALE_RANGE\n",
    "    \n",
    "    np.random.seed(RSEED)\n",
    "    X, y, groups, scalers = getSubsample(EVAL_FRAC * level_os[level] ** EVAL_PWR, level, \n",
    "                                         SCALE_RANGE_TEST, \n",
    "                                         n_repeats = N_REPEATS if level < 15 else N_REPEATS//2, \n",
    "                                         drops=False)\n",
    "    if len(X) == 0:\n",
    "        print(\"No Data for Level {}\".format(level))\n",
    "        continue;\n",
    "        \n",
    "    y_pred, y_true, groups, scaler_stack, X = predictSet(X, y, groups, scalers, clf_set[level]); \n",
    "   # assert (y_true == y.values * scalers.trailing_vol).all()\n",
    "\n",
    "    predictions = pd.DataFrame(y_pred.T, index=y_true.index, columns = QUANTILES)\n",
    "    predictions['y_true'] = y_true.values\n",
    "    predictions = pd.concat((predictions, scaler_stack), axis = 'columns')\n",
    "    predictions['group'] = groups.values\n",
    "    predictions['series'] = X.series\n",
    "    predictions['d'] = X.d\n",
    "    predictions['days_fwd'] = X.days_fwd\n",
    "    \n",
    "    \n",
    "    \n",
    "    losses = pd.DataFrame(index=QUANTILES)\n",
    "    for group in groups.unique():\n",
    "        subpred = predictions[predictions.group == group]\n",
    "        q_losses = []\n",
    "        for quantile in QUANTILES:\n",
    "            q_losses.append((quantile, wspl(subpred.y_true, subpred[quantile], \n",
    "                                  1, subpred.trailing_vol, quantile)))\n",
    "        losses[group] = np.round(pd.DataFrame(q_losses).set_index(0)[1], 4).values\n",
    "    qls[level] = [losses]    \n",
    "    \n",
    "    ramCheck()\n",
    "    \n",
    "    # now combine them\n",
    "    predictions = predictions.groupby(['series', 'd', 'days_fwd']).agg(\n",
    "                dict([(col, 'mean') for col in predictions.columns \n",
    "                          if col not in ['series', 'd', 'days_fwd']]\\\n",
    "                         + [('days_fwd', 'count')])  )\\\n",
    "            .rename(columns = {'days_fwd': 'ct'}).reset_index()\n",
    "    predictions.head()\n",
    "    predictions.sort_values('ct', ascending = False).head(5)\n",
    "    print(len(predictions))\n",
    "    \n",
    "    all_predictions[level] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for level in sorted(all_predictions.keys()):\n",
    "    predictions = all_predictions[level]\n",
    "    \n",
    "    losses = pd.DataFrame(index=LEVEL_QUANTILES[level])\n",
    "    for group in groups.unique():\n",
    "        subpred = predictions[predictions.group == group]\n",
    "        q_losses = []\n",
    "        for quantile in QUANTILES:\n",
    "            q_losses.append((quantile, wspl(subpred.y_true, subpred[quantile], \n",
    "                                  subpred.ct, subpred.trailing_vol, quantile)))\n",
    "        losses[group] = np.round(pd.DataFrame(q_losses).set_index(0)[1], 4).values\n",
    "        \n",
    "        \n",
    "    qls[level] = [losses]\n",
    "    \n",
    "    print(\"\\n\\n\\nLevel {} Year-by-Year OOS Losses for Evaluation Bag {}:\".format(level, 1))\n",
    "    print(losses); #print(); print()\n",
    "        \n",
    "#     print(BAGS)\n",
    "#     print(SS_FRAC)\n",
    "#     print(X.shape); #del X\n",
    "#     print(SCALE_RANGE_TEST)\n",
    "#     print(N_REPEATS)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_predictions[1][all_predictions[1].d == 1912].drop(columns = ['series', 'd', 'group', 'ct'])\\\n",
    "#     .set_index('days_fwd').plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X.dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for level in sorted(all_predictions.keys()):\n",
    "#     print(\"\\nLevel {}:\".format(level))\n",
    "    predictions = all_predictions[level]\n",
    "\n",
    "    predictions['future_d'] = predictions.d + predictions.days_fwd\n",
    "\n",
    "    for quantile in QUANTILES:\n",
    "        true = predictions.y_true\n",
    "        pred = predictions[quantile]\n",
    "        trailing_vol= predictions.trailing_vol\n",
    "\n",
    "        predictions['loss_{}'.format(quantile)] = \\\n",
    "             np.where(true >= pred, \n",
    "                            quantile*(true-pred),\n",
    "                            (1-quantile)*(pred - true) ) / trailing_vol\n",
    "\n",
    "    predictions['loss'] = predictions[[c for c in predictions.columns if 'loss_' in str(c)]].sum(axis = 1)  \n",
    "    predictions['wtg_loss'] = predictions.loss * predictions.ct / predictions.ct.mean()    \n",
    "\n",
    "    # predictions.groupby('series').loss.sum()\n",
    "    # predictions.groupby('series').wtg_loss.sum()\n",
    "    # predictions.groupby('series').wtg_loss.sum().sum()\n",
    "\n",
    "#     predictions.groupby(['series', 'd']).wtg_loss.sum().reset_index().pivot('d', 'series', values='wtg_loss').plot()\n",
    "\n",
    "#     predictions.groupby(['series', 'd']).wtg_loss.sum().reset_index().pivot('d', 'series', values='wtg_loss')\\\n",
    "#             .ewm(span = 7).mean().plot();\n",
    "\n",
    "#     (predictions.groupby(['series', 'future_d']).wtg_loss.sum().reset_index()\\\n",
    "#                 .pivot('future_d', 'series', values='wtg_loss').ewm(span = 7).mean() \\\n",
    "#     ).plot();\n",
    "\n",
    "    # predictions.groupby(['series', 'future_d']).wtg_loss.sum().sort_values(ascending = False) #.ewm(span = 7).mean() \\\n",
    "    # ).plot();\n",
    "    # predictions.groupby(['series', 'future_d']).wtg_loss.sum().sum()\n",
    "\n",
    "#     predictions[(predictions.series == 0) & (predictions.days_fwd < 7 )].groupby('future_d').mean()\\\n",
    "#             [[c for c in predictions.columns if '.' in str(c) and 'loss' not in str(c)]]\\\n",
    "#                 .loc[1550:1700].plot(linewidth = 0.4)\n",
    "#     train_flipped.iloc[:, 1].reset_index(drop=True).loc[1550:1700].plot( linewidth = 1);\n",
    "    # train_flipped.iloc[active_days, 1].iloc[1000:].plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total Time Elapsed: ', (datetime.datetime.now() - start).seconds, 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ramCheck()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# memCheck()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MEM_CAPACITY = 3e6  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_RUNS = 2500 * (1/10 if SPEED or SUPER_SPEED else 1)\n",
    "MIN_RUNS = 20 * (1/20 if SPEED or SUPER_SPEED else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions = {}\n",
    "for level in sorted(list(set(levels.unique()) & set(clf_set.keys()))):\n",
    "    print('\\n\\nCreating Out-of-Sample Predictions for Level {}\\n'.format(level))\n",
    "    \n",
    "    final_base = FINAL_BASE\n",
    "\n",
    "    assert (final_base in ['d_1941', 'd_1913'])\n",
    "    if final_base == 'd_1941':\n",
    "        suffix = 'evaluation'\n",
    "    elif final_base == 'd_1913':\n",
    "        suffix = 'validation'\n",
    "        \n",
    "    print('   predicting 28 days forward from {}'.format(final_base))\n",
    "    final_features = series_features[( series_features.d.map(cal_index_to_day) == final_base) & \n",
    "                                         (series_features.series.map(series_id_level) == level) ]\n",
    "\n",
    "    print('    for {} series'.format(len(final_features)))\n",
    "    \n",
    "    SS_FRAC, SCALE_RANGE = P_DICT[level] # if level < 12 else ID_FILTER]; \n",
    "    SS_FRAC = SS_FRAC * 0.8\n",
    "    print('   scale range of {}'.format(SCALE_RANGE))\n",
    "    \n",
    "    \n",
    "    if level <= 9 or SPEED:\n",
    "        X = []\n",
    "        for df in range(0,28):\n",
    "            Xi = final_features.copy()\n",
    "            Xi['days_fwd'] = df + 1\n",
    "            X.append(Xi)\n",
    "        X = pd.concat(X, ignore_index = True); del Xi; del final_features;\n",
    "\n",
    "        Xn = np.power(X.weights, 2)\n",
    "        Xn = (Xn * MEM_CAPACITY / Xn.sum()).clip(MIN_RUNS, MAX_RUNS)\n",
    "        Xn = (Xn * MEM_CAPACITY / Xn.sum()).clip(MIN_RUNS, MAX_RUNS)\n",
    "        \n",
    "        print('   average repeats: {:.0f}'.format(Xn.mean()))\n",
    "        print('   median repeats: {:.0f}'.format(Xn.median()))\n",
    "        print('   max repeats: {:.0f}'.format(Xn.max()))\n",
    "\n",
    "        X = X.loc[np.repeat(Xn.index, Xn)]\n",
    "\n",
    "        X, y, groups, scalers = getXYG(X, scale_range = SCALE_RANGE, oos = True)\n",
    "        Xd = X.d;  Xseries = X.series\n",
    "        X.drop(columns=['d', 'series'], inplace = True)\n",
    "\n",
    "        print(X.shape)\n",
    "        y_pred = predictOOS(X, scalers, clf_set[level], LEVEL_QUANTILES[level], suffix == 'validation'); print()\n",
    "\n",
    "        predictions = pd.DataFrame(y_pred.T, index=X.index, columns = LEVEL_QUANTILES[level])\n",
    "        predictions = pd.concat((predictions, scalers), axis = 'columns')\n",
    "        predictions['series'] = Xseries\n",
    "        predictions['d'] = Xd\n",
    "        predictions['days_fwd'] = X.days_fwd.astype(np.int8)\n",
    "        predictions['y_true'] = y * scalers.scaler\n",
    "#         break;\n",
    "        ramCheck()\n",
    "\n",
    "        predictions = predictions.groupby(['series', 'd', 'days_fwd']).agg(\n",
    "                        dict([(col, 'mean') for col in predictions.columns \n",
    "                                  if col not in ['series', 'd', 'days_fwd']]\\\n",
    "                                 + [('days_fwd', 'count')])  )\\\n",
    "                    .rename(columns = {'days_fwd': 'ct'}).reset_index()\n",
    "        predictions.days_fwd = predictions.days_fwd.astype(np.int8)\n",
    "\n",
    "    else: # levels 10, 11, 12\n",
    "        \n",
    "        predictions_full = []\n",
    "        \n",
    "        for df in range(0,28):\n",
    "            print( '\\n Predicting {} days forward from {}'.format(df + 1, final_base))\n",
    "            X = final_features.copy()\n",
    "            X['days_fwd'] = df + 1\n",
    "\n",
    "            Xn = np.power(X.weights, 1.5)\n",
    "            Xn = (Xn * MEM_CAPACITY / Xn.sum()).clip(MIN_RUNS, MAX_RUNS)\n",
    "            Xn = (Xn * MEM_CAPACITY / Xn.sum()).clip(MIN_RUNS, MAX_RUNS)\n",
    "            \n",
    "            print('   average repeats: {:.0f}'.format(Xn.mean()))\n",
    "            print('   median repeats: {:.0f}'.format(Xn.median()))\n",
    "            print('   max repeats: {:.0f}'.format(Xn.max()))\n",
    "            \n",
    "            X = X.loc[np.repeat(Xn.index, Xn)]\n",
    "\n",
    "            X, y, groups, scalers = getXYG(X, scale_range = SCALE_RANGE, oos = True)\n",
    "            Xd = X.d;  Xseries = X.series\n",
    "            X.drop(columns=['d', 'series'], inplace = True)\n",
    "\n",
    "            print(X.shape)\n",
    "            y_pred = predictOOS(X, scalers, clf_set[level], LEVEL_QUANTILES[level], suffix == 'validation'); print()\n",
    "\n",
    "            predictions = pd.DataFrame(y_pred.T, index=X.index, columns = LEVEL_QUANTILES[level])\n",
    "            predictions = pd.concat((predictions, scalers), axis = 'columns')\n",
    "            predictions['series'] = Xseries\n",
    "            predictions['d'] = Xd\n",
    "            predictions['days_fwd'] = X.days_fwd.astype(np.int8)\n",
    "            predictions['y_true'] = y * scalers.scaler\n",
    "\n",
    "            ramCheck()\n",
    "\n",
    "            predictions = predictions.groupby(['series', 'd', 'days_fwd']).agg(\n",
    "                            dict([(col, 'mean') for col in predictions.columns \n",
    "                                      if col not in ['series', 'd', 'days_fwd']]\\\n",
    "                                     + [('days_fwd', 'count')])  )\\\n",
    "                        .rename(columns = {'days_fwd': 'ct'}).reset_index()\n",
    "            predictions.days_fwd = predictions.days_fwd.astype(np.int8)\n",
    "            predictions_full.append(predictions)\n",
    "            \n",
    "        predictions = pd.concat(predictions_full); del predictions_full\n",
    " \n",
    "    all_predictions[level] = predictions; del predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('all_predictions_raw.pkl', 'wb') as handle:\n",
    "    pickle.dump(all_predictions, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_predictions = pickle.load(open('../input/m5-submissions/all_predictions_valid_19.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "losses = pd.DataFrame(index=LEVEL_QUANTILES[levels.min()])\n",
    "for level in sorted(all_predictions.keys()):\n",
    "    predictions = all_predictions[level]\n",
    "    subpred = predictions\n",
    "    q_losses = []\n",
    "    for quantile in LEVEL_QUANTILES[level]:\n",
    "        q_losses.append((quantile, wspl(subpred.y_true, subpred[quantile], \n",
    "                              subpred.weights, subpred.trailing_vol, quantile)))\n",
    "\n",
    "#         print(np.round(pd.DataFrame(q_losses).set_index(0)[1], 4).values)\n",
    "    losses[level] = np.round(pd.DataFrame(q_losses).set_index(0)[1], 4).values\n",
    "\n",
    "\n",
    "#         print(\"\\n\\n\\nLevel {} Year-by-Year OOS Losses for Evaluation Bag {}:\".format(level, 1))\n",
    "print(losses); print(); print()\n",
    "print(losses.mean())\n",
    "print(losses.mean().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Level Harmonizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.DataFrame(index = range(1, 29))\n",
    "for level in sorted(all_predictions.keys()):\n",
    "    if level > 9:\n",
    "        continue;\n",
    "    a[level] = all_predictions[level].groupby('days_fwd')[0.5].sum() / level_multiplier[level]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    a.plot()\n",
    "except:\n",
    "    pass;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_predictions[level][quantile]\n",
    "\n",
    "# all_predictions[level][quantile] * all_predictions[level].days_fwd.map(a.mean(axis=1) / a[level] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ADJUSTMENT_FACTOR = 1 if SPEED or SUPER_SPEED else 0.7  # probably better as 1.0, but used 0.7 to be safe;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for level in sorted(all_predictions.keys()):\n",
    "    if level > 9: \n",
    "        continue;\n",
    "        \n",
    "    for quantile in LEVEL_QUANTILES[level]:\n",
    "        all_predictions[level][quantile] = all_predictions[level][quantile] \\\n",
    "                        * ( (1 - ADJUSTMENT_FACTOR) +\n",
    "                              ADJUSTMENT_FACTOR * all_predictions[level].days_fwd.map(  a.mean(axis=1) / a[level] ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.DataFrame(index = range(1, 29))\n",
    "for level in sorted(all_predictions.keys()):\n",
    "    if level > 9:\n",
    "        continue;\n",
    "    a[level] = all_predictions[level].groupby('days_fwd')[0.5].sum() / level_multiplier[level]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    a.plot()\n",
    "except:\n",
    "    pass;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "losses = pd.DataFrame(index=LEVEL_QUANTILES[level])\n",
    "for level in sorted(all_predictions.keys()):\n",
    "    predictions = all_predictions[level]\n",
    "    subpred = predictions\n",
    "    q_losses = []\n",
    "    for quantile in LEVEL_QUANTILES[level]:\n",
    "        q_losses.append((quantile, wspl(subpred.y_true, subpred[quantile], \n",
    "                              subpred.weights, subpred.trailing_vol, quantile)))\n",
    "\n",
    "#         print(np.round(pd.DataFrame(q_losses).set_index(0)[1], 4).values)\n",
    "    losses[level] = np.round(pd.DataFrame(q_losses).set_index(0)[1], 4).values\n",
    "\n",
    "\n",
    "#         print(\"\\n\\n\\nLevel {} Year-by-Year OOS Losses for Evaluation Bag {}:\".format(level, 1))\n",
    "print(losses); print(); print()\n",
    "print(losses.mean())\n",
    "print(losses.mean().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if suffix == 'validation':\n",
    "\n",
    "    losses = pd.DataFrame(index=LEVEL_QUANTILES[level])\n",
    "    for level in sorted(all_predictions.keys()):\n",
    "        predictions = all_predictions[level]\n",
    "        subpred = predictions\n",
    "        q_losses = []\n",
    "        for quantile in LEVEL_QUANTILES[level]:\n",
    "            q_losses.append((quantile, wspl(subpred.y_true, subpred[quantile], \n",
    "                                  subpred.weights, subpred.trailing_vol, quantile)))\n",
    "        \n",
    "        losses[level] = np.round(pd.DataFrame(q_losses).set_index(0)[1], 4).values\n",
    "\n",
    "\n",
    "#         print(\"\\n\\n\\nLevel {} Year-by-Year OOS Losses for Evaluation Bag {}:\".format(level, 1))\n",
    "    print(losses); print(); print()\n",
    "    print(losses.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if suffix == 'validation':\n",
    "    losses.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for level in sorted(all_predictions.keys()):\n",
    "    predictions = all_predictions[level]\n",
    "    (predictions.groupby('days_fwd')[0.5].sum() / level_multiplier[level]).plot(legend = True, \n",
    "                                                                                label = level,\n",
    "                                                                               linewidth = 0.5)\n",
    "    \n",
    "if suffix=='validation':\n",
    "    ( predictions.groupby('days_fwd').y_true.sum() / level_multiplier[level]) .plot(linewidth = 1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_flipped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (series_features[( series_features.d.map(cal_index_to_day) == final_base) & \n",
    "#                                          (series_features.series.map(series_id_level) == level) ]\\\n",
    "#         .sort_values('weights', ascending = False).reset_index().weights.astype(np.float32) ** 1.5).cumsum().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for level in sorted(all_predictions.keys()):\n",
    "    predictions = all_predictions[level]\n",
    "    \n",
    "    if level <= 9:\n",
    "        series_list = predictions.series.unique()[:5]\n",
    "    else:\n",
    "        series_list =  series_features[( series_features.d.map(cal_index_to_day) == final_base) & \n",
    "                                         (series_features.series.map(series_id_level) == level) ]\\\n",
    "            .sort_values('weights', ascending = False).series.to_list()\\\n",
    "                 [:len(predictions.series.unique())//20 : len(predictions.series.unique()) // 500]\n",
    "    \n",
    "    for series in series_list:\n",
    "        \n",
    "        DAYS_BACK = 60\n",
    "        if suffix == 'evaluation':\n",
    "            prior = train_flipped.iloc[-DAYS_BACK:, series]\n",
    "            prior.index = range(-DAYS_BACK + 1, 1 )\n",
    "        else:\n",
    "            prior = train_flipped.iloc[-DAYS_BACK:, series]\n",
    "            prior.index = range(-DAYS_BACK + 28 + 1, 28 + 1 )\n",
    "            \n",
    "            \n",
    "        f = prior.plot( linewidth = 1.5);\n",
    "\n",
    "        f = predictions[predictions.series == series].set_index('days_fwd')\\\n",
    "                [[c for c in predictions.columns if c in LEVEL_QUANTILES[level]]].plot(\n",
    "                                title = (\"Level {} - {}\".format(level, series_id_to_series[series])\n",
    "                                      + (\"\" if level <=9 else \" - weight of {:.2%}\".format(\n",
    "                                          predictions[predictions.series == series].weights.mean() )))\n",
    "                                                       , \n",
    "                                              linewidth = 0.5, ax = f);\n",
    "        f = plt.figure();\n",
    "#     break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_rows = []\n",
    "for level in sorted(all_predictions.keys()):\n",
    "    predictions = all_predictions[level]\n",
    "    df = predictions[ ['series', 'days_fwd'] + list(LEVEL_QUANTILES[level])].copy()\n",
    "    df.series = df.series.map(series_id_to_series)\n",
    "    df = df.melt(['series', 'days_fwd'], var_name = 'q' )\n",
    "    df.value = df.value / level_multiplier[level]\n",
    "    df['name'] = df.series + '_' + df.q.apply(lambda x: '{0:.3f}'.format(x)) + '_' + suffix\n",
    "    # df.days_fwd = 'F' + df.days_fwd.astype(str)\n",
    "\n",
    "    for q in df.q.unique():\n",
    "        qdf = df[df.q==q].pivot('name', 'days_fwd', 'value')\n",
    "        qdf.columns = ['F{}'.format(c) for c in qdf.columns]\n",
    "        qdf.index.name = 'id'\n",
    "        output_rows.append(qdf)\n",
    "    output = pd.concat(output_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(set(output.index) - set(sample_sub.id)) == 0\n",
    "\n",
    "assert len(set(sample_sub.id) & set(output.index)) == len(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = ('submission_{}_lvl_{}.csv'.format(suffix, LEVEL) if MAX_LEVEL == None \n",
    "                                else 'submission_{}_lt_{}.csv'.format(suffix, MAX_LEVEL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.round(3).to_csv(output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(output) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total Time Elapsed: ', (datetime.datetime.now() - start).seconds, 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
